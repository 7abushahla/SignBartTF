{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ec7cd2-76d8-446a-8778-8e5af40a5ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763022133.174622 1422696 cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/f25mappteam8/anaconda3/envs/asl_1/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:335: UserWarning: Device capability of jax unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "WARNING:2025-11-13 12:22:16,128:jax._src.xla_bridge:794: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import ai_edge_torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import pickle\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78829dfd-e22d-461b-8888-0a223918fd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85182f5c-fa13-4271-bae7-9d3c7c1df90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro RTX 4000\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877d1127-e284-4c16-9a56-0204d35f462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "from model import SignBart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a7bb2f8-d8a1-4241-b8bb-c485f3fb6a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignBart(\n",
       "  (encoder): Encoder(\n",
       "    (embed_positions): PositionalEmbedding(258, 144)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x EncoderLayer(\n",
       "        (self_attn): SelfAttention(\n",
       "          (k_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (v_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (q_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (out_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (fc1): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (fc2): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (final_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed_positions): PositionalEmbedding(258, 144)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x DecoderLayer(\n",
       "        (self_attn): CausalSelfAttention(\n",
       "          (k_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (v_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (q_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (out_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELU(approximate='none')\n",
       "        (self_attn_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): CrossAttention(\n",
       "          (k_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (v_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (q_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (out_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (fc2): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (final_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classification_head): ClassificationHead(\n",
       "    (dropout): Dropout(p=0.7, inplace=False)\n",
       "    (out_proj): Linear(in_features=144, out_features=10, bias=True)\n",
       "  )\n",
       "  (projection): Projection(\n",
       "    (proj_x1): Linear(in_features=100, out_features=144, bias=True)\n",
       "    (proj_y1): Linear(in_features=100, out_features=144, bias=True)\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Load config\n",
    "config_path = \"configs/arabic-asl.yaml\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 4. Create model\n",
    "signbart = SignBart(config)\n",
    "signbart.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0546d29-0405-4845-8c66-7cf22da448dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded checkpoint from epoch 79\n",
      "‚úÖ All weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = \"checkpoints_arabic_asl_LOSO_user08/checkpoints_79_final.pth\"  \n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "# The checkpoint contains: {'model': ..., 'optimizer': ..., 'epoch': ...}\n",
    "# Extract just the model state dict\n",
    "if 'model' in checkpoint:\n",
    "    state_dict = checkpoint['model']\n",
    "    print(f\"‚úÖ Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "# Load into model\n",
    "ret = signbart.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "if ret.missing_keys:\n",
    "    print(f\"‚ö†Ô∏è  Missing keys: {len(ret.missing_keys)} keys\")\n",
    "if ret.unexpected_keys:\n",
    "    print(f\"‚ö†Ô∏è  Unexpected keys: {len(ret.unexpected_keys)} keys\")\n",
    "    \n",
    "if not ret.missing_keys and not ret.unexpected_keys:\n",
    "    print(\"‚úÖ All weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a9eeeb5-221d-49ae-946d-35f66879aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Loading real test sample...\n",
      "‚úÖ Sample found: data/arabic-asl_LOSO_user11/test/G10/user11_G10_R10.pkl\n",
      "\n",
      "üìä Sample contents:\n",
      "   Keys: dict_keys(['keypoints', 'class', 'user', 'num_keypoints', 'keypoint_structure', 'video_file', 'num_frames', 'original_path'])\n",
      "   Class: G10\n",
      "   Keypoints shape: (45, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. Load Real Test Sample\n",
    "# ============================================================================\n",
    "print(\"\\nüì¶ Loading real test sample...\")\n",
    "\n",
    "# Load the actual pickle file\n",
    "sample_path = (\"data/arabic-asl_LOSO_user11/test/G10/user11_G10_R10.pkl\")\n",
    "\n",
    "if not os.path.exists(sample_path):\n",
    "    print(f\"‚ùå Sample not found: {sample_path}\")\n",
    "    # Try relative path\n",
    "    sample_path = \"data/arabic-asl_LOSO_user11/test/G10/user11_G10_R10.pkl\"\n",
    "    if os.path.exists(sample_path):\n",
    "        print(f\"‚úÖ Found at relative path: {sample_path}\")\n",
    "    else:\n",
    "        print(\"Please provide correct path to the sample file\")\n",
    "else:\n",
    "    print(f\"‚úÖ Sample found: {sample_path}\")\n",
    "\n",
    "# Load the pickle file\n",
    "with open(sample_path, \"rb\") as f:\n",
    "    sample = pickle.load(f)\n",
    "\n",
    "print(f\"\\nüìä Sample contents:\")\n",
    "print(f\"   Keys: {sample.keys()}\")\n",
    "print(f\"   Class: {sample['class']}\")\n",
    "print(f\"   Keypoints shape: {np.array(sample['keypoints']).shape}\")\n",
    "\n",
    "# Extract keypoints and label\n",
    "keypoints = np.array(sample['keypoints'])[:, :, :2]  # (T, K, 2) - x,y coords only\n",
    "sample_class = sample['class']  # 'G10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ac2bd4-8aee-4398-97ba-416b39b0ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keypoints and label\n",
    "keypoints = np.array(sample['keypoints'])[:, :, :2]  # (T, K, 2) - x,y coords only\n",
    "sample_class = sample['class']  # 'G10'\n",
    "\n",
    "# Load label mappings from the dataset directory\n",
    "dataset_root =  \"data/arabic-asl_LOSO_user11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573b7260-a611-41ae-882f-89a75eb7709d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label: G10 ‚Üí ID: 9\n",
      "   Label mapping: {'0': 'G01', '1': 'G02', '2': 'G03', '3': 'G04', '4': 'G05', '5': 'G06', '6': 'G07', '7': 'G08', '8': 'G09', '9': 'G10'}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{dataset_root}/label2id.json\", 'r') as f:\n",
    "    label2id = json.load(f)\n",
    "\n",
    "with open(f\"{dataset_root}/id2label.json\", 'r') as f:\n",
    "    id2label = json.load(f)\n",
    "\n",
    "sample_label_id = label2id[sample_class]\n",
    "\n",
    "print(f\"   Label: {sample_class} ‚Üí ID: {sample_label_id}\")\n",
    "print(f\"   Label mapping: {id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf77702d-6f3c-494e-b24e-2fd0d13f6e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Final keypoints shape: (45, 100, 2)\n",
      "   Normalized keypoints\n",
      "\n",
      "‚úÖ Sample prepared:\n",
      "   Keypoints: torch.Size([45, 100, 2]) (T=45, K=100)\n",
      "   Label: 9 (G10)\n"
     ]
    }
   ],
   "source": [
    "# Filter keypoints to the joints specified in config (if needed)\n",
    "if 'joint_idxs' in config and config['joint_idxs'] is not None:\n",
    "    flat_joint_idxs = []\n",
    "    for group in config['joint_idxs']:\n",
    "        flat_joint_idxs.extend(group)\n",
    "    flat_joint_idxs = sorted(flat_joint_idxs)\n",
    "    keypoints = keypoints[:, flat_joint_idxs, :]\n",
    "    print(f\"   Filtered to {len(flat_joint_idxs)} keypoints\")\n",
    "\n",
    "# Clip keypoints to [0, 1]\n",
    "keypoints = np.clip(keypoints, 0, 1)\n",
    "\n",
    "# Clip to max 64 frames (as done in dataset.py)\n",
    "if keypoints.shape[0] > 64:\n",
    "    # Simple uniform sampling\n",
    "    indices = np.linspace(0, keypoints.shape[0] - 1, 64, dtype=int)\n",
    "    keypoints = keypoints[indices]\n",
    "    print(f\"   Clipped from {keypoints.shape[0]} to 64 frames\")\n",
    "\n",
    "print(f\"   Final keypoints shape: {keypoints.shape}\")\n",
    "\n",
    "# Normalize keypoints (grouped normalization as in dataset.py)\n",
    "def normalize_keypoints(keypoints, joint_idxs):\n",
    "    \"\"\"Normalize keypoints by groups.\"\"\"\n",
    "    if joint_idxs is None:\n",
    "        return keypoints\n",
    "    \n",
    "    flat_joint_idxs = []\n",
    "    for group in joint_idxs:\n",
    "        flat_joint_idxs.extend(group)\n",
    "    idx_to_pos = {idx: pos for pos, idx in enumerate(sorted(flat_joint_idxs))}\n",
    "    \n",
    "    for i in range(keypoints.shape[0]):  # for each frame\n",
    "        for group in joint_idxs:  # for each group\n",
    "            filtered_positions = [idx_to_pos[idx] for idx in group if idx in idx_to_pos]\n",
    "            \n",
    "            if len(filtered_positions) > 0:\n",
    "                group_keypoints = keypoints[i, filtered_positions, :]\n",
    "                \n",
    "                # Normalize the group\n",
    "                x_coords = group_keypoints[:, 0]\n",
    "                y_coords = group_keypoints[:, 1]\n",
    "                \n",
    "                min_x, min_y = np.min(x_coords), np.min(y_coords)\n",
    "                max_x, max_y = np.max(x_coords), np.max(y_coords)\n",
    "                \n",
    "                w = max_x - min_x\n",
    "                h = max_y - min_y\n",
    "                \n",
    "                if w > h:\n",
    "                    delta_x = 0.05 * w\n",
    "                    delta_y = delta_x + ((w - h) / 2)\n",
    "                else:\n",
    "                    delta_y = 0.05 * h\n",
    "                    delta_x = delta_y + ((h - w) / 2)\n",
    "                \n",
    "                s_point = [max(0, min(min_x - delta_x, 1)), max(0, min(min_y - delta_y, 1))]\n",
    "                e_point = [max(0, min(max_x + delta_x, 1)), max(0, min(max_y + delta_y, 1))]\n",
    "                \n",
    "                # Normalize\n",
    "                if (e_point[0] - s_point[0]) != 0.0:\n",
    "                    group_keypoints[:, 0] = (group_keypoints[:, 0] - s_point[0]) / (e_point[0] - s_point[0])\n",
    "                if (e_point[1] - s_point[1]) != 0.0:\n",
    "                    group_keypoints[:, 1] = (group_keypoints[:, 1] - s_point[1]) / (e_point[1] - s_point[1])\n",
    "                \n",
    "                keypoints[i, filtered_positions, :] = group_keypoints\n",
    "    \n",
    "    return keypoints\n",
    "\n",
    "keypoints = normalize_keypoints(keypoints, config.get('joint_idxs'))\n",
    "print(f\"   Normalized keypoints\")\n",
    "\n",
    "# Convert to torch tensors\n",
    "sample_keypoints = torch.from_numpy(keypoints).float()\n",
    "sample_label = torch.tensor(sample_label_id, dtype=torch.long)\n",
    "\n",
    "print(f\"\\n‚úÖ Sample prepared:\")\n",
    "print(f\"   Keypoints: {sample_keypoints.shape} (T={sample_keypoints.shape[0]}, K={sample_keypoints.shape[1]})\")\n",
    "print(f\"   Label: {sample_label.item()} ({sample_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "744f0831-97ad-445f-9f34-510c7c730f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SignBartInference(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for SignBart that only returns logits (no loss)\n",
    "    This is needed for TFLite conversion since ai_edge_torch doesn't handle None values\n",
    "    \"\"\"\n",
    "    def __init__(self, signbart_model):\n",
    "        super().__init__()\n",
    "        self.model = signbart_model\n",
    "    \n",
    "    def forward(self, keypoints, attention_mask):\n",
    "        # Call original model without labels\n",
    "        loss, logits = self.model(keypoints, attention_mask, labels=None)\n",
    "        # Return only logits (not loss which is None)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71b539bd-5f07-421b-b82f-180331254ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignBartInference(\n",
       "  (model): SignBart(\n",
       "    (encoder): Encoder(\n",
       "      (embed_positions): PositionalEmbedding(258, 144)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (v_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (q_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (out_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELU(approximate='none')\n",
       "          (fc1): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (fc2): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (final_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (embed_positions): PositionalEmbedding(258, 144)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x DecoderLayer(\n",
       "          (self_attn): CausalSelfAttention(\n",
       "            (k_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (v_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (q_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (out_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELU(approximate='none')\n",
       "          (self_attn_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): CrossAttention(\n",
       "            (k_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (v_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (q_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (out_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (fc2): Linear(in_features=144, out_features=144, bias=True)\n",
       "          (final_layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (classification_head): ClassificationHead(\n",
       "      (dropout): Dropout(p=0.7, inplace=False)\n",
       "      (out_proj): Linear(in_features=144, out_features=10, bias=True)\n",
       "    )\n",
       "    (projection): Projection(\n",
       "      (proj_x1): Linear(in_features=100, out_features=144, bias=True)\n",
       "      (proj_y1): Linear(in_features=100, out_features=144, bias=True)\n",
       "    )\n",
       "    (loss_fn): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Wrap model to return only logits\n",
    "signbart_inference = SignBartInference(signbart)\n",
    "signbart_inference.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c1b70e-e328-4376-97c7-35028ddc8e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Running PyTorch inference...\n",
      "‚úÖ PyTorch inference successful!\n",
      "   True label: G10 (ID: 9)\n",
      "   Predicted: G10 (ID: 9)\n",
      "   Confidence: 92.22%\n",
      "   Loss: 0.0810\n",
      "   Correct: ‚úì\n",
      "\n",
      "   Top-5 predictions:\n",
      "      1. G10: 92.22%\n",
      "      2. G04: 4.34%\n",
      "      3. G07: 2.71%\n",
      "      4. G01: 0.60%\n",
      "      5. G06: 0.10%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. Test PyTorch Inference\n",
    "# ============================================================================\n",
    "print(\"\\nüîÑ Running PyTorch inference...\")\n",
    "\n",
    "# Prepare input\n",
    "keypoints_input = sample_keypoints.unsqueeze(0)  # Add batch dimension: (1, T, K, 2)\n",
    "attention_mask = torch.ones(1, keypoints_input.shape[1], dtype=torch.long)  # (1, T)\n",
    "labels = sample_label.unsqueeze(0)  # (1,)\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss, logits = signbart(\n",
    "        keypoints=keypoints_input,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels\n",
    "    )\n",
    "\n",
    "# Get predictions and confidence\n",
    "predicted_class = logits.argmax(dim=1).item()\n",
    "predicted_label = id2label[str(predicted_class)]\n",
    "true_label = id2label[str(sample_label.item())]\n",
    "\n",
    "# Calculate confidence (softmax probabilities)\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "confidence = probs[0, predicted_class].item() * 100  # Confidence in predicted class\n",
    "true_confidence = probs[0, sample_label.item()].item() * 100  # Confidence in true class\n",
    "\n",
    "print(f\"‚úÖ PyTorch inference successful!\")\n",
    "print(f\"   True label: {true_label} (ID: {sample_label.item()})\")\n",
    "print(f\"   Predicted: {predicted_label} (ID: {predicted_class})\")\n",
    "print(f\"   Confidence: {confidence:.2f}%\")\n",
    "if predicted_class != sample_label.item():\n",
    "    print(f\"   True class confidence: {true_confidence:.2f}%\")\n",
    "print(f\"   Loss: {loss.item():.4f}\")\n",
    "print(f\"   Correct: {'‚úì' if predicted_class == sample_label.item() else '‚úó'}\")\n",
    "\n",
    "# Show top-5 predictions\n",
    "top5_probs, top5_indices = torch.topk(probs[0], min(5, len(id2label)))\n",
    "print(f\"\\n   Top-5 predictions:\")\n",
    "for i, (prob, idx) in enumerate(zip(top5_probs, top5_indices), 1):\n",
    "    label = id2label[str(idx.item())]\n",
    "    print(f\"      {i}. {label}: {prob.item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a32ac32-1112-4214-bb79-964f796e8aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Padded input shape: torch.Size([1, 114, 100, 2])\n",
      "   Attention mask shape: torch.Size([1, 114])\n",
      "   ‚úì Wrapper output shape: torch.Size([1, 10])\n",
      "   ‚úì Wrapper predicted: G10 (ID: 9)\n"
     ]
    }
   ],
   "source": [
    "# Prepare sample inputs for conversion\n",
    "# Note: ai_edge_torch needs fixed sequence length, so we'll pad to 114 (max length)\n",
    "max_seq_len = 114\n",
    "\n",
    "# Pad the real sample to max_seq_len\n",
    "T_current = keypoints_input.shape[1]\n",
    "if T_current < max_seq_len:\n",
    "    padding = torch.zeros(1, max_seq_len - T_current, keypoints_input.shape[2], keypoints_input.shape[3])\n",
    "    keypoints_padded = torch.cat([keypoints_input, padding], dim=1)\n",
    "    mask_padding = torch.zeros(1, max_seq_len - T_current, dtype=torch.long)\n",
    "    attention_mask_padded = torch.cat([attention_mask, mask_padding], dim=1)\n",
    "else:\n",
    "    keypoints_padded = keypoints_input[:, :max_seq_len, :, :]\n",
    "    attention_mask_padded = attention_mask[:, :max_seq_len]\n",
    "\n",
    "print(f\"   Padded input shape: {keypoints_padded.shape}\")\n",
    "print(f\"   Attention mask shape: {attention_mask_padded.shape}\")\n",
    "\n",
    "# Test wrapper with padded input\n",
    "with torch.no_grad():\n",
    "    torch_output = signbart_inference(keypoints_padded, attention_mask_padded)\n",
    "    print(f\"   ‚úì Wrapper output shape: {torch_output.shape}\")\n",
    "    predicted = torch_output.argmax(dim=1).item()\n",
    "    print(f\"   ‚úì Wrapper predicted: {id2label[str(predicted)]} (ID: {predicted})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e02de6d2-9001-492c-86e6-f1c5823dd706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Converting to TFLite (this may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1763022153.378200 1422696 cuda_executor.cc:1783] GPU interconnect information not available: INTERNAL: NVML library doesn't have required functions.\n",
      "W0000 00:00:1763022153.379276 1422696 cuda_executor.cc:1783] GPU interconnect information not available: INTERNAL: NVML library doesn't have required functions.\n",
      "W0000 00:00:1763022153.379485 1422696 cuda_executor.cc:1783] GPU interconnect information not available: INTERNAL: NVML library doesn't have required functions.\n",
      "W0000 00:00:1763022153.381264 1422696 cuda_executor.cc:1783] GPU interconnect information not available: INTERNAL: NVML library doesn't have required functions.\n",
      "W0000 00:00:1763022153.381385 1422696 cuda_executor.cc:1783] GPU interconnect information not available: INTERNAL: NVML library doesn't have required functions.\n",
      "W0000 00:00:1763022153.381520 1422696 cuda_executor.cc:1783] GPU interconnect information not available: INTERNAL: NVML library doesn't have required functions.\n",
      "W0000 00:00:1763022153.437593 1422696 cuda_executor.cc:1783] GPU interconnect information not available: INTERNAL: NVML library doesn't have required functions.\n",
      "W0000 00:00:1763022153.437760 1422696 cuda_executor.cc:1783] GPU interconnect information not available: INTERNAL: NVML library doesn't have required functions.\n",
      "W0000 00:00:1763022153.437908 1422696 cuda_executor.cc:1783] GPU interconnect information not available: INTERNAL: NVML library doesn't have required functions.\n",
      "I0000 00:00:1763022153.437925 1422696 gpu_device.cc:2040] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6663 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbg8u4sim/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbg8u4sim/assets\n",
      "W0000 00:00:1763022154.812164 1422696 tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "W0000 00:00:1763022154.812181 1422696 tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "I0000 00:00:1763022154.812587 1422696 reader.cc:83] Reading SavedModel from: /tmp/tmpbg8u4sim\n",
      "I0000 00:00:1763022154.814062 1422696 reader.cc:52] Reading meta graph with tags { serve }\n",
      "I0000 00:00:1763022154.814069 1422696 reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpbg8u4sim\n",
      "I0000 00:00:1763022154.824683 1422696 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\n",
      "I0000 00:00:1763022154.826349 1422696 loader.cc:236] Restoring SavedModel bundle.\n",
      "I0000 00:00:1763022154.897224 1422696 loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpbg8u4sim\n",
      "I0000 00:00:1763022154.918064 1422696 loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 105484 microseconds.\n",
      "I0000 00:00:1763022154.938868 1422696 dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1763022155.636377 1422696 flatbuffer_export.cc:4160] Estimated count of arithmetic ops: 199.934 M  ops, equivalently 99.967 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ TFLite conversion successful!\n",
      "   Saved to: signbart_arabic_asl_user08_epoch79.tflite\n",
      "   File size: 2.94 MB\n"
     ]
    }
   ],
   "source": [
    "# Convert to TFLite\n",
    "sample_inputs = (keypoints_padded, attention_mask_padded)\n",
    "\n",
    "try:\n",
    "    print(\"\\n   Converting to TFLite (this may take a few minutes)...\")\n",
    "    edge_model = ai_edge_torch.convert(signbart_inference.eval(), sample_inputs)\n",
    "    \n",
    "    # Save TFLite model\n",
    "    output_path = \"signbart_arabic_asl_user08_epoch79.tflite\"\n",
    "    edge_model.export(output_path)\n",
    "    \n",
    "    # Get file size\n",
    "    file_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f\"\\n‚úÖ TFLite conversion successful!\")\n",
    "    print(f\"   Saved to: {output_path}\")\n",
    "    print(f\"   File size: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Conversion failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3602171c-dd9d-4941-aa26-f97f88e86dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing TFLite Model\n",
      "================================================================================\n",
      "\n",
      "üìã TFLite Model Details:\n",
      "   Input 0: shape=[  1 114 100   2], dtype=<class 'numpy.float32'>, name=serving_default_args_0:0\n",
      "   Input 1: shape=[  1 114], dtype=<class 'numpy.int64'>, name=serving_default_args_1:0\n",
      "   Output 0: shape=[ 1 10], dtype=<class 'numpy.float32'>, name=StatefulPartitionedCall:0\n",
      "\n",
      "üîÑ Running TFLite inference on real sample...\n",
      "‚úÖ TFLite inference successful!\n",
      "   True label: G10 (ID: 9)\n",
      "   Predicted: G10 (ID: 9)\n",
      "   Confidence: 92.22%\n",
      "   Correct: ‚úì\n",
      "\n",
      "   Top-5 predictions:\n",
      "      1. G10: 92.22%\n",
      "      2. G04: 4.34%\n",
      "      3. G07: 2.71%\n",
      "      4. G01: 0.60%\n",
      "      5. G06: 0.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f25mappteam8/anaconda3/envs/asl_1/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. Test TFLite Model with Real Sample\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing TFLite Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=output_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input/output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"\\nüìã TFLite Model Details:\")\n",
    "for i, detail in enumerate(input_details):\n",
    "    print(f\"   Input {i}: shape={detail['shape']}, dtype={detail['dtype']}, name={detail['name']}\")\n",
    "for i, detail in enumerate(output_details):\n",
    "    print(f\"   Output {i}: shape={detail['shape']}, dtype={detail['dtype']}, name={detail['name']}\")\n",
    "\n",
    "# Run inference with the same padded sample\n",
    "print(f\"\\nüîÑ Running TFLite inference on real sample...\")\n",
    "interpreter.set_tensor(input_details[0]['index'], keypoints_padded.numpy().astype(np.float32))\n",
    "interpreter.set_tensor(input_details[1]['index'], attention_mask_padded.numpy().astype(np.int64))\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output\n",
    "tflite_logits = interpreter.get_tensor(output_details[0]['index'])[0]  # Remove batch dim\n",
    "\n",
    "# Get predictions\n",
    "tflite_predicted_class = tflite_logits.argmax()\n",
    "tflite_predicted_label = id2label[str(tflite_predicted_class)]\n",
    "\n",
    "# Calculate confidence\n",
    "tflite_probs = np.exp(tflite_logits) / np.exp(tflite_logits).sum()\n",
    "tflite_confidence = tflite_probs[tflite_predicted_class] * 100\n",
    "tflite_true_confidence = tflite_probs[sample_label.item()] * 100\n",
    "\n",
    "print(f\"‚úÖ TFLite inference successful!\")\n",
    "print(f\"   True label: {true_label} (ID: {sample_label.item()})\")\n",
    "print(f\"   Predicted: {tflite_predicted_label} (ID: {tflite_predicted_class})\")\n",
    "print(f\"   Confidence: {tflite_confidence:.2f}%\")\n",
    "if tflite_predicted_class != sample_label.item():\n",
    "    print(f\"   True class confidence: {tflite_true_confidence:.2f}%\")\n",
    "print(f\"   Correct: {'‚úì' if tflite_predicted_class == sample_label.item() else '‚úó'}\")\n",
    "\n",
    "# Show top-5 for TFLite\n",
    "top5_indices = np.argsort(tflite_probs)[-5:][::-1]\n",
    "print(f\"\\n   Top-5 predictions:\")\n",
    "for i, idx in enumerate(top5_indices, 1):\n",
    "    label = id2label[str(idx)]\n",
    "    print(f\"      {i}. {label}: {tflite_probs[idx]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87a68fd0-5535-4fe1-878b-4406556b4e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PyTorch vs TFLite Comparison\n",
      "================================================================================\n",
      "\n",
      "üìä Numerical Accuracy:\n",
      "   Max logit difference: 0.000001\n",
      "   Mean logit difference: 0.000000\n",
      "   Max probability difference: 0.000000\n",
      "   Mean probability difference: 0.000000\n",
      "\n",
      "üéØ Prediction Accuracy:\n",
      "   PyTorch predicted: G10 (ID: 9)\n",
      "   TFLite predicted: G10 (ID: 9)\n",
      "   Same prediction: ‚úì\n",
      "\n",
      "üìà Conversion Quality:\n",
      "   ‚úÖ Excellent! Nearly identical outputs.\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TFLite model ready for deployment: signbart_arabic_asl_user08_epoch79.tflite\n",
      "   Input: (1, 114, 100, 2) keypoints + (1, 114) attention_mask\n",
      "   Output: (1, 10) class logits\n",
      "   File size: 2.94 MB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. Compare PyTorch vs TFLite\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PyTorch vs TFLite Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get PyTorch output for padded input (for fair comparison)\n",
    "with torch.no_grad():\n",
    "    pytorch_logits_padded = signbart_inference(keypoints_padded, attention_mask_padded)[0]\n",
    "    pytorch_probs_padded = torch.softmax(pytorch_logits_padded, dim=0).numpy()\n",
    "\n",
    "# Compare logits\n",
    "max_logit_diff = np.abs(pytorch_logits_padded.numpy() - tflite_logits).max()\n",
    "mean_logit_diff = np.abs(pytorch_logits_padded.numpy() - tflite_logits).mean()\n",
    "\n",
    "# Compare probabilities\n",
    "max_prob_diff = np.abs(pytorch_probs_padded - tflite_probs).max()\n",
    "mean_prob_diff = np.abs(pytorch_probs_padded - tflite_probs).mean()\n",
    "\n",
    "print(f\"\\nüìä Numerical Accuracy:\")\n",
    "print(f\"   Max logit difference: {max_logit_diff:.6f}\")\n",
    "print(f\"   Mean logit difference: {mean_logit_diff:.6f}\")\n",
    "print(f\"   Max probability difference: {max_prob_diff:.6f}\")\n",
    "print(f\"   Mean probability difference: {mean_prob_diff:.6f}\")\n",
    "\n",
    "print(f\"\\nüéØ Prediction Accuracy:\")\n",
    "pytorch_pred = pytorch_logits_padded.argmax().item()\n",
    "print(f\"   PyTorch predicted: {id2label[str(pytorch_pred)]} (ID: {pytorch_pred})\")\n",
    "print(f\"   TFLite predicted: {tflite_predicted_label} (ID: {tflite_predicted_class})\")\n",
    "print(f\"   Same prediction: {'‚úì' if pytorch_pred == tflite_predicted_class else '‚úó'}\")\n",
    "\n",
    "# Overall assessment\n",
    "print(f\"\\nüìà Conversion Quality:\")\n",
    "if max_prob_diff < 0.001:\n",
    "    print(\"   ‚úÖ Excellent! Nearly identical outputs.\")\n",
    "elif max_prob_diff < 0.01:\n",
    "    print(\"   ‚úÖ Very good! Minimal differences.\")\n",
    "elif max_prob_diff < 0.05:\n",
    "    print(\"   ‚ö†Ô∏è  Acceptable. Small differences detected.\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Significant differences. May need investigation.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ TFLite model ready for deployment: {output_path}\")\n",
    "print(f\"   Input: (1, 114, 100, 2) keypoints + (1, 114) attention_mask\")\n",
    "print(f\"   Output: (1, {len(id2label)}) class logits\")\n",
    "print(f\"   File size: {file_size_mb:.2f} MB\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d326d89-2950-49a9-a2d9-e48a981e4bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TFLite Model Parameter Analysis (Corrected)\n",
      "================================================================================\n",
      "\n",
      "üìä Model Statistics:\n",
      "   Total model parameters: 1,494,289\n",
      "   Weight tensors: 160\n",
      "   TFLite file size: 2.94 MB\n",
      "   Actual parameter size: 5.63 MB\n",
      "\n",
      "üìã Top 20 Largest Weight Tensors:\n",
      "    1. [  8 114 114]           103,968 params (  406.12 KB) [float32'>]\n",
      "    2. [  1   8 114 114]       103,968 params (  406.12 KB) [float32'>]\n",
      "    3. [  1   8 114 114]       103,968 params (  406.12 KB) [float32'>]\n",
      "    4. [  1   8 114 114]       103,968 params (  406.12 KB) [float32'>]\n",
      "    5. [  1   8 114 114]       103,968 params (  406.12 KB) [float32'>]\n",
      "    6. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "    7. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "    8. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "    9. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "   10. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "   11. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "   12. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "   13. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "   14. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "   15. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "   16. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "   17. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "   18. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "   19. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "   20. [144 144]                20,736 params (   81.00 KB) [float32'>]\n",
      "\n",
      "üîÑ PyTorch vs TFLite Comparison:\n",
      "   PyTorch parameters: 776,458\n",
      "   TFLite parameters: 1,494,289\n",
      "   PyTorch size (float32): 2.96 MB\n",
      "   TFLite file size: 2.94 MB\n",
      "   ‚ö†Ô∏è  Difference: 92.4%\n",
      "   Compression: 1.01x\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Correct TFLite Parameter Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TFLite Model Parameter Analysis (Corrected)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=output_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get tensor details\n",
    "tensor_details = interpreter.get_tensor_details()\n",
    "\n",
    "# Only count actual model weights (stored in the flatbuffer)\n",
    "# These are the tensors that are initialized from the model file\n",
    "total_params = 0\n",
    "weight_tensors = []\n",
    "\n",
    "for tensor in tensor_details:\n",
    "    # Check if this tensor is a constant (stored in the model file)\n",
    "    # Activation tensors are allocated at runtime and don't contribute to model size\n",
    "    quantization = tensor.get('quantization_parameters', {})\n",
    "    \n",
    "    # A tensor is a model parameter if it's used as input to ops but not an input/output\n",
    "    # and has data stored in the model file\n",
    "    shape = tensor['shape']\n",
    "    name = tensor['name']\n",
    "    \n",
    "    # Skip input/output tensors\n",
    "    if 'serving_default' in name:\n",
    "        continue\n",
    "    \n",
    "    # Check if it's a weight by trying to read it\n",
    "    try:\n",
    "        # Try to get the tensor data - only works for constant tensors (weights)\n",
    "        tensor_idx = tensor['index']\n",
    "        tensor_data = interpreter.tensor(tensor_idx)()\n",
    "        \n",
    "        # If we can read it and it's not empty, it's a weight\n",
    "        if tensor_data is not None and tensor_data.size > 0:\n",
    "            num_elements = tensor_data.size\n",
    "            total_params += num_elements\n",
    "            weight_tensors.append({\n",
    "                'name': name,\n",
    "                'shape': shape,\n",
    "                'params': num_elements,\n",
    "                'dtype': tensor['dtype']\n",
    "            })\n",
    "    except:\n",
    "        # If we can't read it, it's an intermediate tensor, not a weight\n",
    "        pass\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total model parameters: {total_params:,}\")\n",
    "print(f\"   Weight tensors: {len(weight_tensors)}\")\n",
    "\n",
    "# Model size analysis\n",
    "file_size_bytes = os.path.getsize(output_path)\n",
    "file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "print(f\"   TFLite file size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# Actual parameter size (most are quantized/compressed in TFLite)\n",
    "actual_param_bytes = sum(w['params'] * np.dtype(w['dtype']).itemsize for w in weight_tensors)\n",
    "actual_param_mb = actual_param_bytes / (1024 * 1024)\n",
    "print(f\"   Actual parameter size: {actual_param_mb:.2f} MB\")\n",
    "\n",
    "# Show largest actual weight tensors\n",
    "print(f\"\\nüìã Top 20 Largest Weight Tensors:\")\n",
    "weight_tensors_sorted = sorted(weight_tensors, key=lambda x: x['params'], reverse=True)\n",
    "for i, tensor in enumerate(weight_tensors_sorted[:20], 1):\n",
    "    size_kb = tensor['params'] * np.dtype(tensor['dtype']).itemsize / 1024\n",
    "    dtype_name = str(tensor['dtype']).split('.')[-1]\n",
    "    print(f\"   {i:2d}. {str(tensor['shape']):<20} {tensor['params']:>10,} params ({size_kb:>8.2f} KB) [{dtype_name}]\")\n",
    "    if len(tensor['name']) < 80:\n",
    "        print(f\"       {tensor['name']}\")\n",
    "\n",
    "# Compare with PyTorch\n",
    "print(f\"\\nüîÑ PyTorch vs TFLite Comparison:\")\n",
    "pytorch_total = sum(p.numel() for p in signbart.parameters())\n",
    "pytorch_size_mb = pytorch_total * 4 / (1024 * 1024)  # float32\n",
    "\n",
    "print(f\"   PyTorch parameters: {pytorch_total:,}\")\n",
    "print(f\"   TFLite parameters: {total_params:,}\")\n",
    "print(f\"   PyTorch size (float32): {pytorch_size_mb:.2f} MB\")\n",
    "print(f\"   TFLite file size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "if abs(pytorch_total - total_params) / pytorch_total < 0.01:\n",
    "    print(f\"   ‚úÖ Parameter counts match!\")\n",
    "else:\n",
    "    diff_pct = abs(pytorch_total - total_params) / pytorch_total * 100\n",
    "    print(f\"   ‚ö†Ô∏è  Difference: {diff_pct:.1f}%\")\n",
    "\n",
    "compression_ratio = pytorch_size_mb / file_size_mb\n",
    "print(f\"   Compression: {compression_ratio:.2f}x\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abd6d216-48d2-4463-8896-537fb3ec6c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TFLite Model Analysis (File-Based)\n",
      "================================================================================\n",
      "\n",
      "üìä File-Based Analysis:\n",
      "   TFLite file size: 2.94 MB (3,080,628 bytes)\n",
      "   Estimated parameters (file_size / 4): ~770,157\n",
      "\n",
      "üîÑ Comparison:\n",
      "   PyTorch parameters: 776,458\n",
      "   PyTorch size (float32): 2.96 MB\n",
      "   TFLite file size: 2.94 MB\n",
      "   Size ratio: 0.992\n",
      "   ‚úÖ Sizes match! TFLite likely has ~776,458 parameters\n",
      "   (The extra -25,204 bytes are model metadata/structure)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Correct Parameter Count - Direct from File\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TFLite Model Analysis (File-Based)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# The most reliable way: file size divided by bytes per parameter\n",
    "file_size_bytes = os.path.getsize(output_path)\n",
    "file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "\n",
    "# TFLite has overhead (model structure, metadata), but most is weights\n",
    "# For float32 models: roughly file_size / 4 = number of parameters\n",
    "estimated_params_from_size = file_size_bytes // 4\n",
    "\n",
    "print(f\"\\nüìä File-Based Analysis:\")\n",
    "print(f\"   TFLite file size: {file_size_mb:.2f} MB ({file_size_bytes:,} bytes)\")\n",
    "print(f\"   Estimated parameters (file_size / 4): ~{estimated_params_from_size:,}\")\n",
    "\n",
    "# Compare with PyTorch\n",
    "pytorch_total = sum(p.numel() for p in signbart.parameters())\n",
    "pytorch_size_mb = pytorch_total * 4 / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüîÑ Comparison:\")\n",
    "print(f\"   PyTorch parameters: {pytorch_total:,}\")\n",
    "print(f\"   PyTorch size (float32): {pytorch_size_mb:.2f} MB\")\n",
    "print(f\"   TFLite file size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# The file sizes match almost exactly, so the parameter counts should too\n",
    "size_ratio = file_size_mb / pytorch_size_mb\n",
    "print(f\"   Size ratio: {size_ratio:.3f}\")\n",
    "\n",
    "if 0.95 <= size_ratio <= 1.05:\n",
    "    print(f\"   ‚úÖ Sizes match! TFLite likely has ~{pytorch_total:,} parameters\")\n",
    "    print(f\"   (The extra {file_size_bytes - pytorch_total*4:,} bytes are model metadata/structure)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Size mismatch - investigating...\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
